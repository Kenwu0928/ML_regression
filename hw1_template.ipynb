{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s109000114/ML_regression/blob/main/hw1_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "basic_trained = np.array([])\n",
        "basic_validate = np.array([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read hw1_basic_training.csv\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "# Read hw1_basic_testing.csv\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prints the number of rows\n",
        "print(training_datalist.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fqv-wFn4c-_",
        "outputId": "17e3620e-55f7-4526-afdf-abc68c404ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_datalist[1]) # header\n",
        "print(training_datalist[1]) # first row of data\n",
        "print(training_datalist[373])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVnFRqX84k-2",
        "outputId": "f5071cf4-fa2a-430a-e342-60bc18d64a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92' '142']\n",
            "['92' '142']\n",
            "['94' '154']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  # seperate basic input as training and validation set\n",
        "  global basic_trained, basic_validate\n",
        "  basic_trained = np.array(training_datalist[1:343])\n",
        "  basic_validate = np.array(training_datalist[343:])\n",
        "  # print(basic_trained[:,0]) # column 0 -> dbp, from the first row to the last row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def findBound(data):\n",
        "  dataMean = np.mean(data)\n",
        "  dataStd = np.std(data)\n",
        "  dataUpper = dataMean + 2 * dataStd\n",
        "  dataLower = dataMean - 2 * dataStd\n",
        "  # print(dataMean)\n",
        "  # print(dataStd)\n",
        "  return dataUpper, dataLower\n",
        "\n",
        "def removeOutlier(data):\n",
        "\n",
        "  # print(data[:][0])\n",
        "  # tmp = data[:,0]\n",
        "  featureUpper, featureLower = findBound(data[:,0])\n",
        "  outputUpper, outputLower = findBound(data[:,1])\n",
        "  # print(\"featureUpper: \", featureUpper)\n",
        "  # print(\"featureLower\", featureLower)\n",
        "  # print(\"outputLower\", outputLower)\n",
        "  # print(\"outputUpper: \", outputUpper)\n",
        "\n",
        "  condition = (data[:, 0] < featureUpper) & (data[:, 0] > featureLower) \\\n",
        "          & (data[:, 1] < outputUpper) & (data[:, 1] > outputLower)\n",
        "\n",
        "  remainingData = data[condition]\n",
        "\n",
        "  return remainingData\n",
        "\n",
        "def PreprocessData():\n",
        "  global basic_trained, basic_validate\n",
        "  basic_trained = basic_trained.astype('float64')\n",
        "  basic_validate = basic_validate.astype('float64')\n",
        "\n",
        "  basic_trained = removeOutlier(basic_trained)\n",
        "  # print('basic_trained : ')\n",
        "  # print(basic_trained)\n",
        "\n",
        "  basic_validate = removeOutlier(basic_validate)\n",
        "  # print('basic_validate : ')\n",
        "  # print(basic_validate)\n",
        "\n",
        "# PreprocessData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "# def MatrixInversion():\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():\n",
        "  global basic_trained, basic_validate\n",
        "\n",
        "  tmp = np.ones((basic_trained[:,0].reshape(-1, 1).shape[0], 1))\n",
        "  basicTrainedDbp = np.concatenate((tmp, basic_trained[:,0].reshape(-1, 1)), axis=1) # two dimensional\n",
        "  # print(basicTrainedDbp)\n",
        "  inverseBasicTrainedDbp = np.linalg.pinv(basicTrainedDbp)\n",
        "  ans = np.dot(inverseBasicTrainedDbp, basic_trained[:,1])\n",
        "  # print(ans)\n",
        "  return ans[1], ans[0]\n",
        "\n",
        "# MakePrediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write output to output_datalist"
      ],
      "metadata": {
        "id": "YW_iBs23dFfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def WriteOutputToList():\n",
        "  global basic_trained, basic_validate, output_datalist, testing_datalist\n",
        "  float_testing_datalist = testing_datalist[1:,].astype('float64')\n",
        "  for i in float_testing_datalist[:, 0]:\n",
        "    output_datalist.append([round(48.50083248 + i * 0.97811663)])\n",
        "  # print(output_datalist)"
      ],
      "metadata": {
        "id": "_hehLDRldcCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCL92EPKOFIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a451e6-d4a3-41e9-e73f-384d6fafd36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9781166254259199 48.500832478436806\n"
          ]
        }
      ],
      "source": [
        "SplitData()\n",
        "PreprocessData()\n",
        "co1, co2 = MakePrediction()\n",
        "print(f'{co1} {co2}')\n",
        "WriteOutputToList()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test validation set MAPE\n",
        "def testValidationMape():\n",
        "  global basic_trained, basic_validate\n",
        "  tmpAns = []\n",
        "  mapeVal = 0\n",
        "  for i in basic_validate[:, 0]:\n",
        "    tmpAns.append(48.48096161325025 + i * 0.9789982101532443)\n",
        "    # print(48.50083248 + i * 0.9789982101532443)\n",
        "\n",
        "  for i in range(len(tmpAns)):\n",
        "    mapeVal += abs((basic_validate[i, 1] - round(tmpAns[i])) / basic_validate[i, 1])\n",
        "  print(mapeVal / len(tmpAns) * 100)\n",
        "  # print(mapeVal / range(len(tmpAns)))\n",
        "\n",
        "testValidationMape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaRMojJpxhmq",
        "outputId": "10bf8fbc-97f8-413e-f746-6173db22f01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.227110740318262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "# training_datalist =  [] # Training datalist, saved as numpy array\n",
        "# testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "gd_output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "# def SplitData():\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "# def PreprocessData():\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-635Ee00YHTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8b51e4-568b-49b1-d5da-97119b428922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[48.48, 0.9], [48.48047038081597, 0.9391990068695709], [48.48070750272228, 0.9589474576963941], [48.480827108831846, 0.9688967219205958], [48.48088751050414, 0.9739091577136758], [48.48091808499122, 0.9764344202395595], [48.48093363254752, 0.977706645325039], [48.480941609538526, 0.9783475903755651], [48.48094577247966, 0.9786704966591097], [48.480948013902946, 0.9788331750696015], [48.480949287265425, 0.9789151307067459], [48.48095007291886, 0.9789564182253396], [48.48095061286413, 0.9789772171531941], [48.48095102902143, 0.9789876939327589], [48.480951382814, 0.9789929704158935], [48.48095170518689, 0.9789956269906354], [48.480952011730146, 0.9789969636560084], [48.480952310298186, 0.9789976353512854], [48.48095260484796, 0.9789979720347518], [48.48095289737304, 0.9789981399393343], [48.48095318887774, 0.9789982228131482], [48.48095347986808, 0.9789982628484809], [48.48095377059885, 0.9789982813017977], [48.48095406119863, 0.9789982888821032], [48.48095435173204, 0.9789982909845912], [48.480954642231644, 0.97899829032736], [48.480954932713935, 0.9789982882797869], [48.48095522318712, 0.9789982855317639], [48.480955513655395, 0.9789982824308583], [48.480955804120896, 0.9789982791521749], [48.48095609458462, 0.9789982757839294], [48.48095638504721, 0.9789982723705677], [48.48095667550887, 0.97899826893448], [48.48095696596968, 0.9789982654869471], [48.48095725642978, 0.9789982620336525], [48.48095754688919, 0.9789982585774571], [48.48095783734793, 0.978998255119806], [48.48095812780598, 0.9789982516614246], [48.48095841826331, 0.9789982482026793], [48.48095870871998, 0.9789982447437557], [48.48095899917596, 0.9789982412847458], [48.48095928963131, 0.9789982378256963], [48.48095958008603, 0.97899823436663], [48.48095987054009, 0.9789982309075602], [48.48096016099345, 0.9789982274484917], [48.48096045144612, 0.9789982239894284], [48.480960741898166, 0.9789982205303714], [48.480961032349555, 0.9789982170713202], [48.480961322800226, 0.9789982136122786], [48.48096161325025, 0.9789982101532443]]\n"
          ]
        }
      ],
      "source": [
        "def loss(params):\n",
        "  global basic_trained, basic_validate\n",
        "  basic_trained_feature = basic_trained[:,0]\n",
        "  basic_trained_output = basic_trained[:,1]\n",
        "  num_samples = len(basic_trained_feature)\n",
        "  loss_sum = 0.0\n",
        "\n",
        "  for x, y in zip(basic_trained_feature,basic_trained_output):\n",
        "    y_hat = np.dot(params, np.array([1.0, x]))\n",
        "    loss_sum += (y_hat - y) ** 2\n",
        "\n",
        "  loss = loss_sum / (num_samples * 2)\n",
        "  return loss\n",
        "\n",
        "def GradientDescent(params, alpha, max_iter):\n",
        "  global basic_trained, basic_validate, coefficient_output\n",
        "  basic_trained_feature = basic_trained[:,0]\n",
        "  basic_trained_output = basic_trained[:,1]\n",
        "  iteration = 0\n",
        "  num_samples = len(basic_trained_feature)\n",
        "  cost = np.zeros(max_iter)\n",
        "  params_store = np.zeros([2, max_iter])\n",
        "\n",
        "  while iteration < max_iter:\n",
        "    cost[iteration] = loss(params)\n",
        "    params_store[:, iteration] = params\n",
        "\n",
        "    # print('======================')\n",
        "    # print(f'iteratin: {iteration}')\n",
        "    # print(f'cost: {cost[iteration]}')\n",
        "\n",
        "    # print('======================')\n",
        "    # print(f'iteratin: {iteration}')\n",
        "    # print(f'coefficients: {params[0]} {params[1]}')\n",
        "    coefficient_output.append([params[0], params[1]])\n",
        "\n",
        "    for x, y in zip(basic_trained_feature, basic_trained_output):\n",
        "      y_hat = np.dot(params, np.array([1.0, x]))\n",
        "      gradient = np.array([1.0, x]) * (y - y_hat)\n",
        "      params += alpha * gradient/num_samples\n",
        "\n",
        "    iteration += 1\n",
        "  print(coefficient_output)\n",
        "  # return params\n",
        "  # return params, cost, params_store\n",
        "\n",
        "# sm = 1000\n",
        "# a = 47.0\n",
        "# b = 0.6\n",
        "# fa = 0.0\n",
        "# fb = 0.0\n",
        "\n",
        "# while a < 50:\n",
        "#   b = 0.6\n",
        "#   while b < 1.0:\n",
        "#     tmp = GradientDescent(np.array([a, b]), 0.0001, 50)\n",
        "#     print('tmp: ', tmp)\n",
        "#     if tmp < sm:\n",
        "#       sm = tmp\n",
        "#       fa = a\n",
        "#       fb = b\n",
        "#     b += 0.1\n",
        "#   a += 0.1\n",
        "\n",
        "# print('sm: ', sm)\n",
        "# print('a: ',a)\n",
        "# print('b: ',b)\n",
        "\n",
        "GradientDescent(np.array([48.48, 0.9]), 0.0001, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90EisOc7YG-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b81fb6-82e8-40c4-f42b-a1d85396d2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[141], [131], [130], [147], [109], [145], [138], [138], [132], [126], [125], [128], [118], [125], [140], [135], [152], [141], [142], [149]]\n"
          ]
        }
      ],
      "source": [
        "# GradientDescent(np.array([48.48, 0.9]), 0.0001, 50)\n",
        "# print(testing_datalist)\n",
        "tmp_testing_datalist = testing_datalist[1:,].astype('float64')\n",
        "for i in tmp_testing_datalist[:, 0]:\n",
        "  gd_output_datalist.append([round(48.48096161325025 + i * 0.9789982101532443)])\n",
        "print(gd_output_datalist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in gd_output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNdwprN8YnJt"
      },
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "advance_training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "advance_testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "advance_output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "advance_training_datalist =  [] # Training datalist, saved as numpy array\n",
        "advance_testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "advance_output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "advance_trained = np.array([])\n",
        "advance_validate = np.array([])\n",
        "\n",
        "seperate_id_data = []\n",
        "seperate_train = []\n",
        "seperate_validate = []\n",
        "\n",
        "id_11526383 = []\n",
        "id_11526383_train = []\n",
        "id_11526383_validate = []\n",
        "\n",
        "id_12923910 = []\n",
        "id_12923910_train = []\n",
        "id_12923910_validate = []\n",
        "\n",
        "id_14699420 = []\n",
        "id_14699420_train = []\n",
        "id_14699420_validate = []\n",
        "\n",
        "id_15437705 = []\n",
        "id_15437705_train = []\n",
        "id_15437705_validate = []\n",
        "\n",
        "id_15642911 = []\n",
        "id_15642911_train = []\n",
        "id_15642911_validate = []\n",
        "\n",
        "id_16298357 = []\n",
        "id_16298357_train = []\n",
        "id_16298357_validate = []\n",
        "\n",
        "id_17331999 = []\n",
        "id_17331999_train = []\n",
        "id_17331999_validate = []\n",
        "\n",
        "id_17593883 = []\n",
        "id_17593883_train = []\n",
        "id_17593883_validate = []\n",
        "\n",
        "id_18733920 = []\n",
        "id_18733920_train = []\n",
        "id_18733920_validate = []\n",
        "\n",
        "id_18791093 = []\n",
        "id_18791093_train = []\n",
        "id_18791093_validate = []\n",
        "\n",
        "id_19473413 = []\n",
        "id_19473413_train = []\n",
        "id_19473413_validate = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOIzdAWxYnJt"
      },
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWv3kxt7YnJt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "95a626c4-f5fe-42d7-9ee3-13d534489bbd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-9ea4d359b0c9>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseperate_id_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0madvance_training_datalist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvance_training_datalist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# AdvanceSplitData()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'subject_id'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Read hw1_advance_training.csv, need to split into train and validate\n",
        "with open(advance_training_dataroot, newline='') as csvfile:\n",
        "  advance_training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "# Read hw1_advance_testing.csv\n",
        "with open(advance_testing_dataroot, newline='') as csvfile:\n",
        "  advance_testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "def AdvanceSplitData(): #TODO: 1\n",
        "  global advance_training_datalist, advance_testing_datalist, advance_output_datalist\n",
        "  global advance_trained, advance_validate, seperate_id_data, seperate_train, seperate_validate\n",
        "  global id_11526383, id_12923910, id_14699420, id_15437705, id_15642911, id_16298357\n",
        "  global id_17331999, id_17593883, id_18733920, id_18791093, id_19473413\n",
        "  identity = ['subject_id', 'charttime', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp']\n",
        "  df = pd.DataFrame(advance_training_datalist, columns=identity)\n",
        "  grouped = df.groupby('subject_id')\n",
        "  sort_by_subject_id = [group_df.values.tolist() for _, group_df in grouped]\n",
        "  # print(sort_by_subject_id[1][1][2])\n",
        "  # lengths = [len(lst) for lst in sort_by_subject_id]\n",
        "  # print(lengths)\n",
        "  array_11526383 = np.array(sort_by_subject_id[0])\n",
        "  id_11526383 = pd.DataFrame(array_11526383, columns=identity)\n",
        "\n",
        "  array_12923910 = np.array(sort_by_subject_id[1])\n",
        "  id_12923910 = pd.DataFrame(array_12923910, columns=identity)\n",
        "\n",
        "  array_14699420 = np.array(sort_by_subject_id[2])\n",
        "  id_14699420 = pd.DataFrame(array_14699420, columns=identity)\n",
        "\n",
        "  array_15437705 = np.array(sort_by_subject_id[3])\n",
        "  id_15437705 = pd.DataFrame(array_15437705, columns=identity)\n",
        "\n",
        "  array_15642911 = np.array(sort_by_subject_id[4])\n",
        "  id_15642911 = pd.DataFrame(array_15642911, columns=identity)\n",
        "\n",
        "  array_16298357 = np.array(sort_by_subject_id[5])\n",
        "  id_16298357 = pd.DataFrame(array_16298357, columns=identity)\n",
        "\n",
        "  array_17331999 = np.array(sort_by_subject_id[6])\n",
        "  id_17331999 = pd.DataFrame(array_17331999, columns=identity)\n",
        "\n",
        "  array_17593883 = np.array(sort_by_subject_id[7])\n",
        "  id_17593883 = pd.DataFrame(array_17593883, columns=identity)\n",
        "\n",
        "  array_18733920 = np.array(sort_by_subject_id[8])\n",
        "  id_18733920 = pd.DataFrame(array_18733920, columns=identity)\n",
        "\n",
        "  array_18791093 = np.array(sort_by_subject_id[9])\n",
        "  id_18791093 = pd.DataFrame(array_18791093, columns=identity)\n",
        "\n",
        "  array_19473413 = np.array(sort_by_subject_id[10])\n",
        "  id_19473413 = pd.DataFrame(array_19473413, columns=identity)\n",
        "  # print(id_11526383)\n",
        "  # print(id_12923910)\n",
        "  seperate_id_data.append(id_11526383)\n",
        "  seperate_id_data.append(id_12923910)\n",
        "  seperate_id_data.append(id_14699420)\n",
        "  seperate_id_data.append(id_15437705)\n",
        "  seperate_id_data.append(id_15642911)\n",
        "  seperate_id_data.append(id_16298357)\n",
        "  seperate_id_data.append(id_17331999)\n",
        "  seperate_id_data.append(id_17593883)\n",
        "  seperate_id_data.append(id_18733920)\n",
        "  seperate_id_data.append(id_18791093)\n",
        "  seperate_id_data.append(id_19473413)\n",
        "  # for i in seperate_id_data:\n",
        "  #   print(i)\n",
        "\n",
        "def DelEmptyData():\n",
        "  global advance_training_datalist, advance_testing_datalist, advance_output_datalist\n",
        "  global advance_trained, advance_validate, seperate_id_data, seperate_train, seperate_validate\n",
        "  global id_11526383, id_12923910, id_14699420, id_15437705, id_15642911, id_16298357\n",
        "  global id_17331999, id_17593883, id_18733920, id_18791093, id_19473413\n",
        "  for i in seperate_id_data:\n",
        "    i = i.dropna(subset=['temperature'])\n",
        "\n",
        "  # print(seperate_id_data[0]['temperature'][1])\n",
        "  # for index, row in seperate_id_data[0].iterrows():\n",
        "  #   for item in row:\n",
        "  #     print(item)\n",
        "  for column in seperate_id_data[0].columns:\n",
        "    seperate_id_data[0] = seperate_id_data[0][seperate_id_data[0][column].notnull()]\n",
        "  # ff = seperate_id_data[0].dropna()\n",
        "  print(seperate_id_data[0])\n",
        "\n",
        "# AdvanceSplitData()\n",
        "# DelEmptyData()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQs6JEDqYnJt"
      },
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym3gp47BYnJt"
      },
      "outputs": [],
      "source": [
        "with open(advance_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in advance_output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}