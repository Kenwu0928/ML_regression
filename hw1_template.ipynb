{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s109000114/ML_regression/blob/main/hw1_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "basic_trained = np.array([])\n",
        "basic_validate = np.array([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read hw1_basic_training.csv\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "# Read hw1_basic_testing.csv\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prints the number of rows\n",
        "print(training_datalist.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fqv-wFn4c-_",
        "outputId": "59481903-0be3-466c-d016-975090dd0fe3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_datalist[1]) # header\n",
        "print(training_datalist[1]) # first row of data\n",
        "print(training_datalist[373])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVnFRqX84k-2",
        "outputId": "18537cc0-7060-4c48-9148-48d3378ad17d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['92' '142']\n",
            "['92' '142']\n",
            "['94' '154']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  # seperate basic input as training and validation set\n",
        "  global basic_trained, basic_validate\n",
        "  basic_trained = np.array(training_datalist[1:343])\n",
        "  basic_validate = np.array(training_datalist[343:])\n",
        "  # print(basic_trained[:,0]) # column 0 -> dbp, from the first row to the last row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def findBound(data):\n",
        "  dataMean = np.mean(data)\n",
        "  dataStd = np.std(data)\n",
        "  dataUpper = dataMean + 2 * dataStd\n",
        "  dataLower = dataMean - 2 * dataStd\n",
        "  # print(dataMean)\n",
        "  # print(dataStd)\n",
        "  return dataUpper, dataLower\n",
        "\n",
        "def removeOutlier(data):\n",
        "\n",
        "  # print(data[:][0])\n",
        "  # tmp = data[:,0]\n",
        "  featureUpper, featureLower = findBound(data[:,0])\n",
        "  outputUpper, outputLower = findBound(data[:,1])\n",
        "  # print(\"featureUpper: \", featureUpper)\n",
        "  # print(\"featureLower\", featureLower)\n",
        "  # print(\"outputLower\", outputLower)\n",
        "  # print(\"outputUpper: \", outputUpper)\n",
        "\n",
        "  condition = (data[:, 0] < featureUpper) & (data[:, 0] > featureLower) \\\n",
        "          & (data[:, 1] < outputUpper) & (data[:, 1] > outputLower)\n",
        "\n",
        "  remainingData = data[condition]\n",
        "\n",
        "  return remainingData\n",
        "\n",
        "def PreprocessData():\n",
        "  global basic_trained, basic_validate\n",
        "  basic_trained = basic_trained.astype('float64')\n",
        "  basic_validate = basic_validate.astype('float64')\n",
        "\n",
        "  basic_trained = removeOutlier(basic_trained)\n",
        "  # print('basic_trained : ')\n",
        "  # print(basic_trained)\n",
        "\n",
        "  basic_validate = removeOutlier(basic_validate)\n",
        "  # print('basic_validate : ')\n",
        "  # print(basic_validate)\n",
        "\n",
        "# PreprocessData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "# def MatrixInversion():\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():\n",
        "  global basic_trained, basic_validate\n",
        "\n",
        "  tmp = np.ones((basic_trained[:,0].reshape(-1, 1).shape[0], 1))\n",
        "  basicTrainedDbp = np.concatenate((tmp, basic_trained[:,0].reshape(-1, 1)), axis=1) # two dimensional\n",
        "  # print(basicTrainedDbp)\n",
        "  inverseBasicTrainedDbp = np.linalg.pinv(basicTrainedDbp)\n",
        "  ans = np.dot(inverseBasicTrainedDbp, basic_trained[:,1])\n",
        "  # print(ans)\n",
        "  return ans[1], ans[0]\n",
        "\n",
        "# MakePrediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write output to output_datalist"
      ],
      "metadata": {
        "id": "YW_iBs23dFfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def WriteOutputToList():\n",
        "  global basic_trained, basic_validate, output_datalist, testing_datalist\n",
        "  float_testing_datalist = testing_datalist[1:,].astype('float64')\n",
        "  for i in float_testing_datalist[:, 0]:\n",
        "    output_datalist.append([round(48.50083248 + i * 0.97811663)])\n",
        "  # print(output_datalist)"
      ],
      "metadata": {
        "id": "_hehLDRldcCr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iCL92EPKOFIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57809dcd-b96d-4664-aa39-d76dddbad225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9781166254259199 48.500832478436806\n"
          ]
        }
      ],
      "source": [
        "SplitData()\n",
        "PreprocessData()\n",
        "co1, co2 = MakePrediction()\n",
        "print(f'{co1} {co2}')\n",
        "WriteOutputToList()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test validation set MAPE\n",
        "def testValidationMape():\n",
        "  global basic_trained, basic_validate\n",
        "  tmpAns = []\n",
        "  mapeVal = 0\n",
        "  for i in basic_validate[:, 0]:\n",
        "    tmpAns.append(48.48096161325025 + i * 0.9789982101532443)\n",
        "    # print(48.50083248 + i * 0.9789982101532443)\n",
        "\n",
        "  for i in range(len(tmpAns)):\n",
        "    mapeVal += abs((basic_validate[i, 1] - round(tmpAns[i])) / basic_validate[i, 1])\n",
        "  print(mapeVal / len(tmpAns) * 100)\n",
        "  # print(mapeVal / range(len(tmpAns)))\n",
        "\n",
        "testValidationMape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaRMojJpxhmq",
        "outputId": "10bf8fbc-97f8-413e-f746-6173db22f01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.227110740318262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "# training_datalist =  [] # Training datalist, saved as numpy array\n",
        "# testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "gd_output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "# def SplitData():\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "# def PreprocessData():\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-635Ee00YHTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8b51e4-568b-49b1-d5da-97119b428922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[48.48, 0.9], [48.48047038081597, 0.9391990068695709], [48.48070750272228, 0.9589474576963941], [48.480827108831846, 0.9688967219205958], [48.48088751050414, 0.9739091577136758], [48.48091808499122, 0.9764344202395595], [48.48093363254752, 0.977706645325039], [48.480941609538526, 0.9783475903755651], [48.48094577247966, 0.9786704966591097], [48.480948013902946, 0.9788331750696015], [48.480949287265425, 0.9789151307067459], [48.48095007291886, 0.9789564182253396], [48.48095061286413, 0.9789772171531941], [48.48095102902143, 0.9789876939327589], [48.480951382814, 0.9789929704158935], [48.48095170518689, 0.9789956269906354], [48.480952011730146, 0.9789969636560084], [48.480952310298186, 0.9789976353512854], [48.48095260484796, 0.9789979720347518], [48.48095289737304, 0.9789981399393343], [48.48095318887774, 0.9789982228131482], [48.48095347986808, 0.9789982628484809], [48.48095377059885, 0.9789982813017977], [48.48095406119863, 0.9789982888821032], [48.48095435173204, 0.9789982909845912], [48.480954642231644, 0.97899829032736], [48.480954932713935, 0.9789982882797869], [48.48095522318712, 0.9789982855317639], [48.480955513655395, 0.9789982824308583], [48.480955804120896, 0.9789982791521749], [48.48095609458462, 0.9789982757839294], [48.48095638504721, 0.9789982723705677], [48.48095667550887, 0.97899826893448], [48.48095696596968, 0.9789982654869471], [48.48095725642978, 0.9789982620336525], [48.48095754688919, 0.9789982585774571], [48.48095783734793, 0.978998255119806], [48.48095812780598, 0.9789982516614246], [48.48095841826331, 0.9789982482026793], [48.48095870871998, 0.9789982447437557], [48.48095899917596, 0.9789982412847458], [48.48095928963131, 0.9789982378256963], [48.48095958008603, 0.97899823436663], [48.48095987054009, 0.9789982309075602], [48.48096016099345, 0.9789982274484917], [48.48096045144612, 0.9789982239894284], [48.480960741898166, 0.9789982205303714], [48.480961032349555, 0.9789982170713202], [48.480961322800226, 0.9789982136122786], [48.48096161325025, 0.9789982101532443]]\n"
          ]
        }
      ],
      "source": [
        "def loss(params):\n",
        "  global basic_trained, basic_validate\n",
        "  basic_trained_feature = basic_trained[:,0]\n",
        "  basic_trained_output = basic_trained[:,1]\n",
        "  num_samples = len(basic_trained_feature)\n",
        "  loss_sum = 0.0\n",
        "\n",
        "  for x, y in zip(basic_trained_feature,basic_trained_output):\n",
        "    y_hat = np.dot(params, np.array([1.0, x]))\n",
        "    loss_sum += (y_hat - y) ** 2\n",
        "\n",
        "  loss = loss_sum / (num_samples * 2)\n",
        "  return loss\n",
        "\n",
        "def GradientDescent(params, alpha, max_iter):\n",
        "  global basic_trained, basic_validate, coefficient_output\n",
        "  basic_trained_feature = basic_trained[:,0]\n",
        "  basic_trained_output = basic_trained[:,1]\n",
        "  iteration = 0\n",
        "  num_samples = len(basic_trained_feature)\n",
        "  cost = np.zeros(max_iter)\n",
        "  params_store = np.zeros([2, max_iter])\n",
        "\n",
        "  while iteration < max_iter:\n",
        "    cost[iteration] = loss(params) # just for debugging\n",
        "    params_store[:, iteration] = params\n",
        "\n",
        "    # print('======================')\n",
        "    # print(f'iteratin: {iteration}')\n",
        "    # print(f'cost: {cost[iteration]}')\n",
        "\n",
        "    # print('======================')\n",
        "    # print(f'iteratin: {iteration}')\n",
        "    # print(f'coefficients: {params[0]} {params[1]}')\n",
        "    coefficient_output.append([params[0], params[1]])\n",
        "\n",
        "    for x, y in zip(basic_trained_feature, basic_trained_output):\n",
        "      y_hat = np.dot(params, np.array([1.0, x]))\n",
        "      gradient = np.array([1.0, x]) * (y - y_hat)\n",
        "      params += alpha * gradient/num_samples\n",
        "\n",
        "    iteration += 1\n",
        "  print(coefficient_output)\n",
        "  # return params\n",
        "  # return params, cost, params_store\n",
        "\n",
        "# sm = 1000\n",
        "# a = 47.0\n",
        "# b = 0.6\n",
        "# fa = 0.0\n",
        "# fb = 0.0\n",
        "\n",
        "# while a < 50:\n",
        "#   b = 0.6\n",
        "#   while b < 1.0:\n",
        "#     tmp = GradientDescent(np.array([a, b]), 0.0001, 50)\n",
        "#     print('tmp: ', tmp)\n",
        "#     if tmp < sm:\n",
        "#       sm = tmp\n",
        "#       fa = a\n",
        "#       fb = b\n",
        "#     b += 0.1\n",
        "#   a += 0.1\n",
        "\n",
        "# print('sm: ', sm)\n",
        "# print('a: ',a)\n",
        "# print('b: ',b)\n",
        "\n",
        "GradientDescent(np.array([48.48, 0.9]), 0.0001, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90EisOc7YG-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b81fb6-82e8-40c4-f42b-a1d85396d2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[141], [131], [130], [147], [109], [145], [138], [138], [132], [126], [125], [128], [118], [125], [140], [135], [152], [141], [142], [149]]\n"
          ]
        }
      ],
      "source": [
        "# GradientDescent(np.array([48.48, 0.9]), 0.0001, 50)\n",
        "# print(testing_datalist)\n",
        "tmp_testing_datalist = testing_datalist[1:,].astype('float64')\n",
        "for i in tmp_testing_datalist[:, 0]:\n",
        "  gd_output_datalist.append([round(48.48096161325025 + i * 0.9789982101532443)])\n",
        "print(gd_output_datalist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in gd_output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNdwprN8YnJt"
      },
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "advance_training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "advance_testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "advance_output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "advance_training_datalist =  [] # Training datalist, saved as numpy array\n",
        "advance_testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "advance_output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "# advance_trained = np.array([])\n",
        "# advance_validate = np.array([])\n",
        "\n",
        "# seperate_id_data = pd.DataFrame()\n",
        "# seperate_train = pd.DataFrame()\n",
        "# seperate_validate = pd.DataFrame()\n",
        "\n",
        "id_11526383 = pd.DataFrame()\n",
        "id_11526383_train = pd.DataFrame()\n",
        "id_11526383_validate = pd.DataFrame()\n",
        "\n",
        "id_12923910 = pd.DataFrame()\n",
        "id_12923910_train = pd.DataFrame()\n",
        "id_12923910_validate = pd.DataFrame()\n",
        "\n",
        "id_14699420 = pd.DataFrame()\n",
        "id_14699420_train = pd.DataFrame()\n",
        "id_14699420_validate = pd.DataFrame()\n",
        "\n",
        "id_15437705 = pd.DataFrame()\n",
        "id_15437705_train = pd.DataFrame()\n",
        "id_15437705_validate = pd.DataFrame()\n",
        "\n",
        "id_15642911 = pd.DataFrame()\n",
        "id_15642911_train = pd.DataFrame()\n",
        "id_15642911_validate = pd.DataFrame()\n",
        "\n",
        "id_16298357 = pd.DataFrame()\n",
        "id_16298357_train = pd.DataFrame()\n",
        "id_16298357_validate = pd.DataFrame()\n",
        "\n",
        "id_17331999 = pd.DataFrame()\n",
        "id_17331999_train = pd.DataFrame()\n",
        "id_17331999_validate = pd.DataFrame()\n",
        "\n",
        "id_17593883 = pd.DataFrame()\n",
        "id_17593883_train = pd.DataFrame()\n",
        "id_17593883_validate = pd.DataFrame()\n",
        "\n",
        "id_18733920 = pd.DataFrame()\n",
        "id_18733920_train = pd.DataFrame()\n",
        "id_18733920_validate = pd.DataFrame()\n",
        "\n",
        "id_18791093 = pd.DataFrame()\n",
        "id_18791093_train = pd.DataFrame()\n",
        "id_18791093_validate = pd.DataFrame()\n",
        "\n",
        "id_19473413 = pd.DataFrame()\n",
        "id_19473413_train = pd.DataFrame()\n",
        "id_19473413_validate = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOIzdAWxYnJt"
      },
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SWv3kxt7YnJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913d6502-8833-46b8-da99-beac53bf1bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# with open(advance_training_dataroot, newline='') as csvfile:\n",
        "#   advance_training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "# with open(advance_testing_dataroot, newline='') as csvfile:\n",
        "#   advance_testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "\n",
        "training_df = pd.read_csv(advance_training_dataroot)\n",
        "training_df = training_df.dropna()\n",
        "training_df = training_df.reset_index(drop=True)\n",
        "grouped = training_df.groupby('subject_id')\n",
        "training_df = [group for _, group in grouped]\n",
        "training_df = {value: group for value, group in grouped}\n",
        "\n",
        "# for index, row in training_df[11526383].iterrows():\n",
        "#     print(row[1])\n",
        "\n",
        "for index in range(2, len(id_11526383)):\n",
        "  current_row = id_11526383.iloc[index]\n",
        "  last_row = id_11526383.iloc[index - 1]\n",
        "  last_2nd_row = id_11526383.iloc[index - 2]\n",
        "  # print(current_row, last_row, last_2nd_row)\n",
        "\n",
        "# print(id_11526383.iloc[4][3]) # important info\n",
        "\n",
        "testing_df = pd.read_csv(advance_testing_dataroot)\n",
        "testing_df = testing_df.dropna()\n",
        "testing_df = testing_df.reset_index(drop=True)\n",
        "\n",
        "def AdvanceSplitData(data): #TODO: 1\n",
        "  global advance_training_datalist, advance_testing_datalist, advance_output_datalist\n",
        "  global advance_trained, advance_validate, seperate_id_data, seperate_train, seperate_validate\n",
        "  global id_11526383, id_12923910, id_14699420, id_15437705, id_15642911, id_16298357\n",
        "  global id_17331999, id_17593883, id_18733920, id_18791093, id_19473413\n",
        "  # identity = ['subject_id', 'charttime', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp']\n",
        "  data_len = len(data)\n",
        "  train_size = int(0.85 * data_len)\n",
        "\n",
        "  return data[:train_size], data[train_size : ]\n",
        "\n",
        "\n",
        "id_11526383 = training_df[11526383]\n",
        "id_11526383_train, id_11526383_validate = AdvanceSplitData(id_11526383)\n",
        "\n",
        "# print(f'{len(id_11526383), len(id_11526383_train), len(id_11526383_validate)}')\n",
        "\n",
        "id_12923910 = training_df[12923910]\n",
        "id_12923910_train, id_12923910_validate = AdvanceSplitData(id_12923910)\n",
        "# print(f'{len(id_12923910), len(id_12923910_train), len(id_12923910_validate)}')\n",
        "\n",
        "id_14699420 = training_df[14699420]\n",
        "id_14699420_train, id_14699420_validate = AdvanceSplitData(id_14699420)\n",
        "# print(f'{len(id_14699420), len(id_14699420_train), len(id_14699420_validate)}')\n",
        "\n",
        "id_15437705 = training_df[15437705]\n",
        "id_15437705_train, id_15437705_validate = AdvanceSplitData(id_15437705)\n",
        "# print(f'{len(id_15437705), len(id_15437705_train), len(id_15437705_validate)}')\n",
        "\n",
        "id_15642911 = training_df[15642911]\n",
        "id_15642911_train, id_15642911_validate = AdvanceSplitData(id_15642911)\n",
        "# print(f'{len(id_15642911), len(id_15642911_train), len(id_15642911_validate)}')\n",
        "\n",
        "id_16298357 = training_df[16298357]\n",
        "id_16298357_train, id_16298357_validate = AdvanceSplitData(id_16298357)\n",
        "# print(f'{len(id_16298357), len(id_16298357_train), len(id_16298357_validate)}')\n",
        "\n",
        "id_17331999 = training_df[17331999]\n",
        "id_17331999_train, id_17331999_validate = AdvanceSplitData(id_17331999)\n",
        "# print(f'{len(id_17331999), len(id_17331999_train), len(id_17331999_validate)}')\n",
        "\n",
        "id_17593883 = training_df[17593883]\n",
        "id_17593883_train, id_17593883_validate = AdvanceSplitData(id_17593883)\n",
        "# print(f'{len(id_17593883), len(id_17593883_train), len(id_17593883_validate)}')\n",
        "\n",
        "id_18733920 = training_df[18733920]\n",
        "id_18733920_train, id_18733920_validate = AdvanceSplitData(id_18733920)\n",
        "# print(f'{len(id_18733920), len(id_18733920_train), len(id_18733920_validate)}')\n",
        "\n",
        "id_18791093 = training_df[18791093]\n",
        "id_18791093_train, id_18791093_validate = AdvanceSplitData(id_18791093)\n",
        "# print(f'{len(id_18791093), len(id_18791093_train), len(id_18791093_validate)}')\n",
        "\n",
        "id_19473413 = training_df[19473413]\n",
        "id_19473413_train, id_19473413_validate = AdvanceSplitData(id_19473413)\n",
        "# print(f'{len(id_19473413), len(id_19473413_train), len(id_19473413_validate)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQs6JEDqYnJt"
      },
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym3gp47BYnJt"
      },
      "outputs": [],
      "source": [
        "with open(advance_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in advance_output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}